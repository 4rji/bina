#!/usr/bin/env python3

import queue
import threading
import urllib.error
import urllib.parse
import urllib.request

print("")
print("\033[1;31m  ____ No funciona con https ____\033[0m")
print("")
threads = 50
target_url = input("\033[1;36m    > URL target or 't' to test it: \033[0m")

if target_url.lower() == "t":
    target_url = "http://testphp.vulnweb.com"

def ask_for_download():
    response = input("\033[1;33m¿    > Desea descargar la lista de palabras? [y/N]: \033[0m")
    if response.lower() == "y":
        return True
    else:
        return False

def download_wordlist(wordlist_url):
    try:
        urllib.request.urlretrieve(wordlist_url, "all.txt")
    except urllib.error.URLError as e:
        print("\033[1;31mError al descargar la lista de palabras:\033[0m", e)

def build_wordlist(wordlst_file):
    if ask_for_download():
        download_wordlist("https://raw.githubusercontent.com/4rji/4rji/main/wordlists/all.txt")
    
    # leer la lista de palabras
    fd = open(wordlst_file, "r")
    raw_words = [line.rstrip('\n') for line in fd]
    fd.close()

    found_resume = False
    words = queue.Queue()

    for word in raw_words:
        if resume:
            if found_resume:
                words.put(word)
            else:
                if word == resume:
                    found_resume = True
                    print("\033[1;32mReanudando la lista de palabras desde: %s\033[0m" % resume)
        else:
            words.put(word)
    return words

def dir_bruter(extensions=None):
    while not word_queue.empty():
        attempt = word_queue.get()
        attempt_list = []

        # verificar si hay una extensión de archivo; si no la hay, es una ruta de directorio que estamos probando
        if "." not in attempt:
            attempt_list.append("/%s/" % attempt)
        else:
            attempt_list.append("/%s" % attempt)

        # si queremos probar extensiones
        if extensions:
            for extension in extensions:
                attempt_list.append("/%s%s" % (attempt, extension))

        # iterar sobre nuestra lista de intentos        
        for brute in attempt_list:
            url = "%s%s" % (target_url, urllib.parse.quote(brute))
            try:
                headers = {"User-Agent": user_agent}
                r = urllib.request.Request(url, headers=headers)
                response = urllib.request.urlopen(r)
                if len(response.read()):
                    print("\033[1;34m[%d] => %s\033[0m" % (response.code, url))
            except urllib.error.HTTPError as e:
                if e.code != 404:
                    print("\033[1;31m!!! %d => %s\033[0m" % (e.code, url))
                pass

wordlist_file = "all.txt"  # de SVNDigger
resume = None
user_agent = "Mozilla/5.0 (X11; Linux x86_64; rv:19.0) " \
             "Gecko/20100101 " \
             "Firefox/19.0"

word_queue = build_wordlist(wordlist_file)
file_extensions = [".php", ".bak", ".orig", ".inc"]

for i in range(threads):
    t = threading.Thread(target=dir_bruter, args=(file_extensions,))
    t.start()

